{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c09fece7eca8a3c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Template for CLEM segmentation\n",
    "\n",
    "This template contains the basic steps for CLEM analysis.\n",
    "1. Stitch EM image(s)\n",
    "2. Invert EM image(s)\n",
    "3. Register (correlate) EM and LM images\n",
    "4. Define Cell Mask\n",
    "5. Segment EM images\n",
    "    5.1 Automatically segment EM images using AI (Mask-RCNN)\n",
    "    5.2 Manually correct (or create from scratch) segmentation\n",
    "6. Get statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df559dabc2207c95",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. Stitch EM \n",
    "EM images are aquired in multiple tiles using SerialEM (in the Taraska-Lab at NHLBI, NIH).\n",
    "The first step of image analysis is stitching of the tiles to form a single image. This is done in IMOD using sloppyblend.\n",
    "\n",
    "**--> A custom-written bash script by Dr Sochacki can be pulled from local gitlab repository linux_batch_EMstitching.**\n",
    "\n",
    "Notes:\n",
    " - Must have IMOD installed\n",
    "  - Use IMOD version 4.11 or higher!\n",
    "    In older versions, the pixel spacing is saved in dots/inch instead of dots/centimeter which creates problems when reading pixel size from metadata in the rest of the analysis pipeline.\n",
    "  - If you have an older IMOD version installed, you can circumvent this problem by manually updating the pixel size to Âµm units in FIJI.\n",
    "    Open image in FIJI > Analyze > Set Scale... > set the correct pixel size and unit > OK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c049727229548626",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Invert EM-image(s)\n",
    "Historically, we have used inverted PREM images for CLEM analysis.\n",
    "The ai-based segmentation is trained on inverted images and most of the code in the pyclem package expects inverted images.\n",
    "\n",
    "For further analysis, file names for EM-images should end with \"_inv.tif\"\n",
    "\n",
    "- You can invert images using FIJI:\n",
    "    1. Open image in FIJI\n",
    "    2. Edit > Invert\n",
    "    3. Save image\n",
    "\n",
    "- **Or use the python script for batch-processing included in pyclem (see below)**\n",
    "    </br>\n",
    "    The code uses regular expression patterns to find all files in a directory matching a certain pattern.\n",
    "    --> To (VERY briefly) learn about regular expressions, see: https://www.dataquest.io/blog/regex-cheatsheet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603a053155357229",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pyclem.file_io import get_files\n",
    "from pyclem.scripts import invert_and_save\n",
    "\n",
    "# Define path to directory with EM images\n",
    "parent_directory = r'PATH_to_EM_images'\n",
    "# Define regular expression pattern to match the filenames of YOUR EM-images\n",
    "filename_pattern = r'.*cell.*\\.tif$'\n",
    "\n",
    "# Retrieve names of all files in the directory matching the pattern\n",
    "filenames, _ = get_files(subdir=parent_directory, pattern=filename_pattern)\n",
    "# Add parent directory to filenames to get full paths\n",
    "filenames = [Path(parent_directory, filename) for filename in filenames]\n",
    "# Invert images\n",
    "invert_and_save(fn=filenames)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4045e44b43afdeb7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Register (correlate) EM and LM images\n",
    "For image registration, I recommend using the plugin \"affinder\" for the open-source python image analysis software \"Napari\".\n",
    "\n",
    "Until the plugin is available in the official napari plugin repository, it can be installed from the local gitlab repository \"affinder\".\n",
    "To do so, follow these steps:\n",
    " - Open conda prompt\n",
    " - activate your pyclem environment\n",
    " - Install the plugin directly from github with:\n",
    "   - **python -m pip install git+https://github.com/jni/affinder.git**\n",
    " - Only do this once! The plugin will be available in napari from now on.\n",
    "\n",
    "To use the plugin:\n",
    "1. Open a napari viewer (see code below)\n",
    "2. Open images to register (eg. EM, TIRF and STORM image) using the aicsimageio plugin. This ensures that the images are opened with the correct metadata (most importantly the pixel size).\n",
    "    File > Open with plugin > Open File(s) > Open > select napari-aicsimageio as reader, when prompted\n",
    "3. Open the affinder plugin\n",
    "    Plugins > affinder > Start affinder\n",
    "4. Select EM image as reference and LM image as moving\n",
    "5. (Optional) Select location to save the transformation matrix\n",
    "6. Click **\"Start\"** to start the manual registration process\n",
    "7. Select three points in the reference image\n",
    "8. Select the corresponding three points in the moving image (**in the same order!**)\n",
    "    Affinder will perform a first calculation of the transformation matrix and apply it to the moving image\n",
    "9. From here, you can iteratively add single points to improve the registration\n",
    "10. When you are satisfied, click **\"Finish\"** to save the transformation matrix and exit the process.\n",
    "\n",
    "To copy transformation from one image to another (e.g. from TIRF to STORM):\n",
    "1. Open affinder widget to copy transformation\n",
    "    Plugins > affinder > Copy affine\n",
    "2. Set the already transformed image (e.g. TIRF) as src_layer and the untransformed image (e.g. STORM) as dst_layer\n",
    "3. Click **\"Copy\"** to apply the transformation to the dst_layer\n",
    "4. You can now use the \"Start affinder\" widget to further improve the registration, using the transformed image (e.g. STORM) as moving and the EM-image as reference.\n",
    "\n",
    "To apply the transformation to an image (creating a new image):\n",
    "1. Open affinder widget to apply transformation\n",
    "    Plugins > affinder > Apply affine\n",
    "2. Select the EM-image as reference layer and the transformed LM-image as moving layer\n",
    "3. Click **\"Apply\"** to create a new image with the transformation applied.\n",
    "4. You now use the built in napari functionality to save the new image.\n",
    "    File > Save Selected Layer(s)... > Save (use napari builtins lossless)\n",
    "\n",
    "To load previously saved transformation matrix:\n",
    "1. Open affinder widget to load transformation\n",
    "    Plugins > affinder > Load affine\n",
    "2. Select the image you want to transform (e.g. LM-image) as layer\n",
    "3. Select File to set the path to a saved transformation matrix\n",
    "4. Click **\"Load\"** to apply the transformation to the selected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2abb2c20c4bcae",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import napari\n",
    "\n",
    "viewer = napari.Viewer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370f692f0ecc0352",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. Define Cell Mask\n",
    "Use the custom-written Napari widget cellmask_widget to create and save a binary cell mask.\n",
    "1. Open a napari viewer and add the widget (see code below)\n",
    "2. Select an (inverted) EM image from the widget.\n",
    "3. Click **\"Start\"**\n",
    "   - The image will be opened together with an empty shapes layer called \"Cell Mask\"\n",
    "   - When opening images, cellmask_widget will automatically rescale the image to a pixelsiize of x nm/pixel as defined by the slider.\n",
    "     This is to improve performance and reduce memory usage.\n",
    "4. Use Napari shapes functionalities to cover areas you want to **exlude** from analysis.\n",
    "5. When done, click **\"Finish and Save\"** to translate the shapes into a binary save it as a tif-file with the addendum \"..._cellmask.tif\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e782707b7f50dfe",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyclem.cellmask import cellmask_main\n",
    "\n",
    "cellmask_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487308dcc16e5bb1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5.1 Automatically segment EM images using AI (Mask-RCNN)\n",
    "The segmentation of EM images is done using a Mask-RCNN model trained on manually segmented EM-images.\n",
    "The model is implemented in a custom-written python package called prem-segmentation.\n",
    "**--> see local gitlab repository PREM-seg**\n",
    "\n",
    "To this end, (inverted!) EM images are scaled to a pixelsize of 2.46 nm/px and split into overlapping tiles. Each tiles is then segmented utilizing the open source deep learning algorithm \"Mask R-CNN\". Segmented tiles are then scaled back to the original pixel size and stitched to a full image mask.\n",
    "\n",
    "**NOTE: Make sure to run this next cell in the appropriate environment (mrcnn-env).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0193f756d601a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from prem_seg.main import main\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a3349a9d413e88",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5.2 Manually correct (or create from scratch) segmentation\n",
    "The automatic segmentation is not perfect and often needs manual correction.\n",
    "To this end, use the Napari widget \"checkseg\" included in the pyclem package.\n",
    "\n",
    "1. Open a napari viewer and add the widget (see code below)\n",
    "2. Select an (inverted) EM-image\n",
    "   - The paramter tile size defines the size of image tiles in Âµm. Default is 3x3 Âµm.\n",
    "3. Click **\"Start\"**\n",
    "   - checkseg_widget will automatically open the corresponding segmentation mask (if it exists in the same folder)\n",
    "        - The segmentation mask will be translated into three shapes layers: \"Domes\", \"Flats\", \"Spheres\"\n",
    "   - checkseg_widget will automatically consider the cellmask (if it exists in the same folder)\n",
    "   - checkseg_widget will automatically progress to the first tile containing relevant data (i.e. not excluded by the cellmask, if available)\n",
    "4. Use Napari shapes functionalities to correct segmentation results in current tile\n",
    "5. When done, click **\"Next\"** to progress to the next tile\n",
    "6. When done with all tiles, checkseg_widget will show the whole cell without tiles for a final check.\n",
    "7. When done, click **\"Finish and Save\"** to translate the shapes into an RGB-mask and save it as a tif-file with the addendum \"..._segmask_check.tif\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddc735285e988c9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyclem.checkseg import checkseg_main\n",
    "\n",
    "checkseg_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9415fafe0ac15d1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 6. Get statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103cb670ee192c45",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
